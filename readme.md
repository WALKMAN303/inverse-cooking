# üçΩÔ∏è Inverse Cooking: Recipe Generation from Food Images
> A deep learning project that generates cooking recipes from food images using CNN and Transformer architecture.

---

## üìñ Overview

This project implements an **image-to-text generation model** that can look at a food image and generate step-by-step cooking instructions. Built with PyTorch, it combines a CNN encoder (ResNet18) for visual feature extraction and a Transformer decoder for sequential text generation.

### Key Features
- üñºÔ∏è **Image Understanding**: Uses pre-trained ResNet18 to extract visual features
- üìù **Recipe Generation**: Transformer decoder generates coherent cooking instructions
- üéØ **End-to-End Training**: Single pipeline from image input to text output
- üìä **13,463 Recipes**: Trained on diverse food dataset with real recipes
- ‚ö° **GPU Optimized**: Efficient training with CUDA support

---

## üéØ Problem Statement

**Challenge**: Given an image of a finished dish, automatically generate the cooking recipe.

**Why It Matters**:
- Helps home cooks recreate dishes they see
- Assists people with dietary restrictions in understanding ingredients
- Educational tool for culinary students
- Accessibility feature for recipe discovery

---

## üóÉÔ∏è Architecture

### Model Design

```
Input Image (128√ó128√ó3)
         ‚Üì
    CNN Encoder (ResNet18)
         ‚Üì
    Feature Vector (256)
         ‚Üì
  Transformer Decoder (2 layers, 4 heads)
         ‚Üì
    Recipe Text Output
```

### Components

1. **CNN Encoder**
   - Pre-trained ResNet18 (ImageNet weights)
   - Fine-tuned last layer
   - Output: 256-dimensional feature vector

2. **Transformer Decoder**
   - 2 decoder layers
   - 4 attention heads
   - Embedding dimension: 256
   - Vocabulary size: 8,215 words
   - Max sequence length: 512 tokens

3. **Training Strategy**
   - Teacher forcing during training
   - Auto-regressive generation during inference
   - Cross-entropy loss
   - AdamW optimizer with learning rate scheduling

---

## üìä Dataset

- **Source**: [Food Ingredients and Recipe Dataset](https://www.kaggle.com/datasets/pes12017000148/food-ingredients-and-recipe-dataset-with-images)
- **Total Recipes**: 13,463
- **Train/Val Split**: 80/20
- **Image Size**: 128√ó128 RGB
- **Data Augmentation**: Random crop, horizontal flip, color jitter

---

## üöÄ Installation

### Prerequisites
```bash
Python 3.8+
CUDA 11.0+ (for GPU support)
```

### Quick Start

1. **Clone the repository**
```bash
git clone https://github.com/WALKMAN303/inverse-cooking.git
cd inverse-cooking
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Download pre-trained models** (See [Model Files](#-model-files) section below)
```bash
python download_models.py
```

4. **Run the script**
```bash
python app.py
```

That's it! You're ready to generate recipes from food images! üéâ

---

## üì¶ Model Files

> **Note**: Due to GitHub's file size limitations (100 MB max), the pre-trained model weights are hosted on Google Drive.

### Download Pre-trained Models

**Option 1: Automatic Download (Recommended)**
```bash
python download_models.py
```

**Option 2: Manual Download**

Download the following files and place them in the project root directory:

| File | Size | Description | Download Link |
|------|------|-------------|---------------|
| `best_model.pth` | 118 MB | Best checkpoint model | [Download](https://drive.google.com/file/d/1HfSb-zVBlxTf22YrheEVGbT7KjZHoe-C/view?usp=sharing) |
| `vocab.pkl` | <1 MB | Vocabulary object | [Download](https://drive.google.com/file/d/13S6BM-Fc5uPCO-Tn1VJxxUnx3U0zFlHD/view?usp=sharing) |

**After downloading, your project structure should look like:**
```
inverse-cooking/
‚îú‚îÄ‚îÄ best_model.pth                ‚úÖ
‚îú‚îÄ‚îÄ vocab.pkl                     ‚úÖ
‚îú‚îÄ‚îÄ inverse-cooking.ipynb
‚îú‚îÄ‚îÄ download_models.py
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ image
```

### Alternative: Train Your Own Models

If you prefer to train from scratch instead of using pre-trained weights:
```bash

jupyter notebook inverse-cooking.ipynb

python train.py --epochs 15 --batch-size 32 --lr 3e-4
```

Training takes approximately 1 hours on Tesla T4 GPU and use kaggle or colab for T4 GPU.

---

## üíª Usage

### Quick Inference

```python
from model import InverseCookingModel, generate_recipe
import torch
from PIL import Image

# Load trained model
model = InverseCookingModel(embed_size=256, vocab_size=8215, ...)
model.load_state_dict(torch.load('best_model.pth'))
model.eval()

# Generate recipe from image
image_path = 'test_images/pizza.jpg'
recipe = generate_recipe(image_path, model, vocab, transform, device)
print(recipe)
```

---

## üìà Results

### Training Performance

| Metric | Value |
|--------|-------|
| Best Validation Loss | 2.96 |
| Training Time | ~1 hours (15 epochs on Tesla T4) |
| Final Train Loss | 2.92 |
| Model Parameters | 17.7M |

### Sample Predictions

**Example : Pizza**
- **Generated**: "preheat oven to 450 degrees roll out pizza dough spread tomato sauce add mozzarella cheese top with pepperoni bake for 15 minutes until cheese is melted and bubbly"
- **Quality**: Coherent, logical sequence ‚úÖ

---

## üõ†Ô∏è Technical Details

### Hyperparameters

```python
BATCH_SIZE = 32
LEARNING_RATE = 3e-4
NUM_EPOCHS = 15
EMBED_SIZE = 256
NUM_HEADS = 4
NUM_LAYERS = 2
MAX_SEQ_LENGTH = 512
```

### Model Architecture Details

```
CNNEncoder(
  (resnet): Sequential(...)
  (linear): Linear(512 ‚Üí 256)
  (bn): BatchNorm1d(256)
  (relu): ReLU()
)

TransformerDecoder(
  (embedding): Embedding(8215, 256)
  (positional_encoding): Parameter(512, 256)
  (transformer_decoder): TransformerDecoder(2 layers)
  (fc_out): Linear(256 ‚Üí 8215)
)

Total Parameters: 17,761,623
Trainable Parameters: 7,042,815
```

---

## üìÅ Project Structure

```
inverse-cooking/
‚îú‚îÄ‚îÄ inverse-cooking.ipynb      # Main training notebook
‚îú‚îÄ‚îÄ model.py                   # Model architecture
‚îú‚îÄ‚îÄ download_models.py         # Script to download pre-trained models
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îú‚îÄ‚îÄ best_model.pth             # Trained model weights (download required)
‚îú‚îÄ‚îÄ vocab.pkl                  # Vocabulary object (download required)
‚îú‚îÄ‚îÄ README.md                  # This file
‚îî‚îÄ‚îÄ images/                    # Sample images and results
    ‚îú‚îÄ‚îÄ architecture.png
    ‚îî‚îÄ‚îÄ training_history.png
```

---

## üîß Requirements

```
torch>=2.0.0
torchvision>=0.15.0
pandas>=1.5.0
pillow>=9.0.0
scikit-learn>=1.2.0
matplotlib>=3.5.0
kagglehub>=0.1.0
gdown>=4.7.1
```

---

## üéì Learning Outcomes

This project demonstrates:
- **Deep Learning**: CNN + Transformer architecture
- **Computer Vision**: Image feature extraction with ResNet
- **NLP**: Sequential text generation with attention
- **PyTorch**: Custom datasets, data loaders, training loops
- **Best Practices**: Modular code, documentation, version control
- **MLOps**: Model versioning and external storage for large files

---

## üöß Future Improvements

- [ ] Add ingredient extraction as separate task
- [ ] Implement beam search for better generation
- [ ] Add BLEU/ROUGE metrics for evaluation
- [ ] Fine-tune with larger models (ResNet50, ViT)
- [ ] Create web demo with Gradio/Streamlit
- [ ] Add nutritional information prediction

---

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üë§ Author

**Your Name**
- GitHub: [@WALKMAN303](https://github.com/WALKMAN303)
- LinkedIn: [Your Name](https://linkedin.com/in/arjun-k-r-)
---

## üìû Contact

Have questions or suggestions? Feel free to:
- Open an issue
- Reach out on LinkedIn

---

<div align="center">
  <sub>Built with ‚ù§Ô∏è using PyTorch</sub>
</div>